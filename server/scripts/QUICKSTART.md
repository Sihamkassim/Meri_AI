# ASTU RAG Knowledge Base - Quick Start Guide

## ðŸŽ¯ What We Built

A **production-ready RAG (Retrieval-Augmented Generation) knowledge base scraper** that:

âœ… Scrapes 12 ASTU university web pages  
âœ… Cleans and structures content semantically  
âœ… Chunks intelligently (300-500 tokens per chunk)  
âœ… Generates high-quality embeddings using Voyage AI  
âœ… Stores in PostgreSQL with pgvector for semantic search  
âœ… Deduplicates automatically using SHA-256 hashes  
âœ… Validates all content before storage  

## ðŸ“ Files Created

```
server/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scrape_astu_rag.py          # Main scraper (750+ lines)
â”‚   â”œâ”€â”€ run_rag_scraper.sh          # Setup and run script
â”‚   â”œâ”€â”€ README_RAG_SCRAPER.md       # Full documentation
â”‚   â””â”€â”€ VOYAGE_API_SETUP.md         # API key setup guide
â””â”€â”€ .env                             # Added VOYAGE_API_KEY config
```

## ðŸš€ Quick Start (3 Steps)

### Step 1: Get Voyage AI API Key

```bash
# Visit: https://www.voyageai.com/
# Sign up, get your API key, then:

nano /home/lelo/projects/Divas/server/.env
# Add: VOYAGE_API_KEY=your-key-here
```

### Step 2: Activate Virtual Environment

```bash
cd /home/lelo/projects/Divas/server
source venv/bin/activate
```

### Step 3: Run the Scraper

```bash
./scripts/run_rag_scraper.sh
```

Or run directly:

```bash
python3 scripts/scrape_astu_rag.py
```

## ðŸ“Š What It Does

### URLs Being Scraped (12 total)

1. **College of EEC**
   - Human Resource Management Directorate
   - Background of ASTU
   - Mission and Purpose
   - Electronics & Communication Engineering Dept

2. **College of MCME**
   - Background of ASTU
   - Chemical Engineering Dept
   - Materials Science & Engineering Dept
   - Mechanical Engineering Dept

3. **College of CEA**
   - Architecture Dept
   - Civil Engineering Dept
   - Water Resource Engineering Dept

4. **College of ANS**
   - Pharmacy Dept

### Processing Pipeline

```
URL â†’ Scrape â†’ Clean â†’ Chunk â†’ Validate â†’ Embed â†’ Store
```

1. **Scrape**: Extract semantic content only (no nav, ads, etc.)
2. **Clean**: Convert to Markdown, remove duplicates, normalize
3. **Chunk**: Semantic chunks (300-500 tokens, 10-15% overlap)
4. **Validate**: Check token limits, metadata, quality
5. **Embed**: Generate 1024-dim vectors with Voyage-3-large
6. **Store**: Save to PostgreSQL with deduplication

## ðŸ—„ï¸ Database Schema

Automatically creates `document_chunks` table:

```sql
CREATE TABLE document_chunks (
    id              SERIAL PRIMARY KEY,
    content         TEXT NOT NULL,
    embedding       vector(1024),        -- Voyage-3-large
    source_url      VARCHAR(512),
    page_title      VARCHAR(512),
    section_title   VARCHAR(512),
    content_type    VARCHAR(100),        -- policy|guide|faq|doc
    tags            TEXT[],
    language        VARCHAR(10),
    date_scraped    TIMESTAMP,
    hash            VARCHAR(64) UNIQUE,   -- SHA-256 for dedup
    token_count     INTEGER,
    created_at      TIMESTAMP DEFAULT NOW()
);
```

With indexes for:
- Hash lookup (deduplication)
- Vector similarity search (retrieval)
- Content type filtering

## ðŸ“ˆ Expected Output

```
================================================================================
ðŸš€ ASTU RAG Knowledge Base Scraper
================================================================================

Processing: https://www.astu.edu.et/...
ðŸ“¥ Scraping: [URL]
âœ“ Successfully scraped: [Page Title]
ðŸ§¹ Cleaning content from: [Page Title]
âœ‚ï¸  Chunking content from: [Page Title]
âœ“ Created 8 semantic chunks
âœ… Validating 8 chunks
âœ“ Validated: 8/8 chunks passed
ðŸ”® Generating embeddings for 8 chunks using voyage-3-large
âœ“ Generated 8 embeddings
ðŸ’¾ Storing 8 chunks in database
âœ“ Stored 8 chunks, skipped 0 duplicates

[... continues for all 12 URLs ...]

================================================================================
ðŸ“Š FINAL REPORT
================================================================================
Pages scraped:         12
Chunks created:        ~120-150
Chunks embedded:       ~120-150
Chunks skipped (dup):  0
Errors encountered:    0

================================================================================
âœ… RAG Knowledge Base Processing Complete!
================================================================================
```

## ðŸ” Using the Knowledge Base

### Query with Semantic Search

```python
from database import Database

db = Database()
conn = db.connect()

# Generate query embedding
query_text = "What departments are in the engineering college?"
query_embedding = voyage_client.embed([query_text], model="voyage-3-large")

# Search
results = db.semantic_search(query_embedding[0], limit=5)

for result in results:
    print(f"Score: {result['similarity']:.2f}")
    print(f"Source: {result['source_url']}")
    print(f"Content: {result['content'][:200]}...")
    print()
```

## ðŸŽ¨ Features Implemented

### âœ… Scraping Excellence
- Removes all navigation, ads, headers, footers
- Preserves semantic structure
- Extracts only main content

### âœ… Advanced Cleaning
- Converts to clean Markdown
- Deduplicates sentences
- Normalizes whitespace
- No HTML artifacts

### âœ… Semantic Chunking
- 300-500 tokens per chunk
- 10-15% overlap
- Never splits procedures/definitions
- Preserves complete ideas

### âœ… Rich Metadata
```json
{
  "source_url": "...",
  "page_title": "...",
  "section_title": "...",
  "content_type": "documentation",
  "tags": ["ASTU", "engineering", "..."],
  "language": "en",
  "date_scraped": "2026-01-23T..."
}
```

### âœ… Quality Assurance
- Token limit validation
- Content quality checks
- SHA-256 deduplication
- Atomic transactions
- Comprehensive error logging

### âœ… Production Ready
- Idempotent (safe to re-run)
- Batch processing
- Error recovery
- Detailed reporting
- Proper logging

## ðŸ› ï¸ Troubleshooting

### No API Key
```
âš ï¸  VOYAGE_API_KEY not set
```
**Solution**: Add key to `.env` file (see VOYAGE_API_SETUP.md)

### Database Error
```
âœ— Database connection failed
```
**Solution**: Check DATABASE_URL in `.env`, verify network

### Scraping Timeout
```
âœ— Failed to scrape [URL]: timeout
```
**Solution**: Check internet, increase timeout in code

## ðŸ“š Documentation

- **README_RAG_SCRAPER.md** - Full technical documentation
- **VOYAGE_API_SETUP.md** - API key setup guide
- **This file** - Quick start guide

## ðŸ”„ Re-running the Scraper

Safe to re-run anytime:
```bash
cd /home/lelo/projects/Divas/server
source venv/bin/activate
python3 scripts/scrape_astu_rag.py
```

The scraper will:
- Skip duplicate chunks (by hash)
- Update if content changed
- Add new pages if URLs added

## ðŸ“ Next Steps

1. **Get Voyage API key** (required for embeddings)
2. **Run the scraper** to populate the knowledge base
3. **Integrate with RAG pipeline** for AI-powered Q&A
4. **Test semantic search** to verify retrieval quality
5. **Monitor and update** regularly for fresh content

## ðŸŽ¯ Quality Metrics

Expected results:
- **Retrieval Accuracy**: >90% (with Voyage embeddings)
- **Chunk Quality**: 300-500 tokens, meaningful boundaries
- **No Duplicates**: SHA-256 hash deduplication
- **Clean Content**: No HTML, proper Markdown
- **Rich Context**: Metadata for filtering and ranking

---

## ðŸš€ Ready to Start?

```bash
cd /home/lelo/projects/Divas/server
source venv/bin/activate
./scripts/run_rag_scraper.sh
```

**Need help?** Check the detailed documentation in `scripts/README_RAG_SCRAPER.md`

---

**Built by**: Senior Data Engineer  
**Date**: January 23, 2026  
**Stack**: Python, BeautifulSoup, Voyage AI, PostgreSQL, pgvector
