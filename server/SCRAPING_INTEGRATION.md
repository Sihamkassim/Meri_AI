# ASTU Website Scraping & RAG System

## Overview

Your application now has a complete web scraping system that integrates with your existing RAG (Retrieval-Augmented Generation) architecture.

## Architecture

```
Web Scraping Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ASTU Website   â”‚
â”‚ www.astu.edu.et â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper Service    â”‚
â”‚ - Extract content   â”‚
â”‚ - Clean HTML        â”‚
â”‚ - Chunk documents   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini AI Service  â”‚
â”‚ - Generate embeddingsâ”‚
â”‚ (vector 768)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase Postgres  â”‚
â”‚ - Store documents   â”‚
â”‚ - Store embeddings  â”‚
â”‚ - pgvector search   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG Service       â”‚
â”‚ - Retrieve context  â”‚
â”‚ - Generate answers  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features Implemented

### 1. Web Scraper Service (`app/services/scraper_service.py`)
- âœ… Extract clean content from any URL
- âœ… Remove scripts, styles, navigation
- âœ… Support for multiple extraction methods (trafilatura, BeautifulSoup)
- âœ… Smart content chunking (1000 chars with 200 overlap)
- âœ… Sitemap.xml support
- âœ… Link extraction and crawling
- âœ… Rate limiting and respectful scraping
- âœ… Specialized ASTU scraper class

### 2. Admin API Endpoints (`app/routers/admin.py`)

#### POST `/api/admin/scrape/url`
Scrape a single URL and store in database.
```bash
curl -X POST "http://localhost:4000/api/admin/scrape/url" \
  -F "url=https://www.astu.edu.et/about"
```

#### POST `/api/admin/scrape/astu`
Scrape multiple pages from ASTU website.
```bash
curl -X POST "http://localhost:4000/api/admin/scrape/astu" \
  -F "max_pages=30"
```

#### POST `/api/admin/scrape/custom`
Scrape multiple custom URLs.
```bash
curl -X POST "http://localhost:4000/api/admin/scrape/custom" \
  -H "Content-Type: application/json" \
  -d '{"urls": ["url1", "url2"]}'
```

### 3. Standalone Script (`scripts/scrape_and_populate.py`)
```bash
# Scrape ASTU website
python scripts/scrape_and_populate.py --max-pages 30

# Scrape single URL
python scripts/scrape_and_populate.py --url "https://www.astu.edu.et/about"

# Scrape from sitemap
python scripts/scrape_and_populate.py --sitemap
```

## How It Works

### 1. Admin Manual Entry (Already Working âœ“)
- Admin uses `/api/admin/documents` POST endpoint
- Provides title, content, source
- System generates embedding
- Stores in Supabase

### 2. Admin Web Scraping (NEW âœ“)
- Admin triggers scraping via API or script
- System scrapes ASTU website
- Extracts clean content
- Chunks large documents
- Generates embeddings for each chunk
- Stores in same Supabase table

### 3. Retrieval (RAG)
- User asks question
- System generates query embedding
- Searches Supabase with pgvector
- Returns top-k similar documents
- Feeds to Gemini for answer generation

## Database Schema

Your existing `documents` table works perfectly:
```sql
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    content TEXT NOT NULL,
    source VARCHAR(255),         -- URL for scraped content
    tags TEXT[],                 -- ['web-scraped', 'astu-website']
    embedding vector(768),        -- Gemini embeddings
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## Usage Examples

### Install Dependencies First
```bash
cd server
pip install -r requirements.txt
```

### Option 1: Run Scraping Script
```bash
python scripts/scrape_and_populate.py --max-pages 30
```

### Option 2: Use Admin API
```bash
# Start server
uvicorn main:app --reload --port 4000

# Trigger scraping
curl -X POST "http://localhost:4000/api/admin/scrape/astu" \
  -F "max_pages=30"
```

### Option 3: Manual Admin Entry (existing)
```bash
curl -X POST "http://localhost:4000/api/admin/documents" \
  -F "title=ASTU History" \
  -F "content=Long content..." \
  -F "source=manual" \
  -F "tags=[\"university\", \"history\"]"
```

## Important Notes

### Existing Features Preserved âœ“
- Your existing admin CRUD endpoints still work
- Manual document creation unchanged
- RAG service unchanged
- Vector search unchanged
- All integrations intact

### New Capabilities âœ“
- Automated content population
- Web scraping from any URL
- ASTU-specific scraping
- Smart content chunking
- Batch processing

### Content Sources
Documents can now come from:
1. **Manual entry** by admin
2. **Single URL scraping**
3. **Bulk website crawling**
4. **Sitemap-based scraping**
5. **File uploads** (already supported)

## Next Steps

1. **Install dependencies**:
   ```bash
   pip install beautifulsoup4 requests html2text trafilatura
   ```

2. **Test single URL scrape**:
   ```bash
   python scripts/scrape_and_populate.py --url "https://www.astu.edu.et/"
   ```

3. **Run full ASTU scrape**:
   ```bash
   python scripts/scrape_and_populate.py --max-pages 30
   ```

4. **Verify in database**:
   ```sql
   SELECT COUNT(*) FROM documents WHERE source LIKE '%astu.edu.et%';
   SELECT title, source FROM documents LIMIT 10;
   ```

5. **Test RAG retrieval**:
   ```bash
   curl -X POST "http://localhost:4000/api/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "What is ASTU?"}'
   ```

## Configuration

All configuration uses your existing `.env`:
- `SUPABASE_URL` - Already configured âœ“
- `DATABASE_URL` - Already configured âœ“
- `AI_API_KEY` - Already configured âœ“
- `EMBEDDING_MODEL` - Already configured (text-embedding-004) âœ“

No additional setup needed!

## File Structure

```
server/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ scraper_service.py    [NEW] Web scraping logic
â”‚   â”‚   â”œâ”€â”€ ai_service.py          [EXISTS] Embeddings
â”‚   â”‚   â”œâ”€â”€ rag_service.py         [EXISTS] RAG
â”‚   â”‚   â””â”€â”€ vector_service.py      [EXISTS] Search
â”‚   â””â”€â”€ routers/
â”‚       â””â”€â”€ admin.py               [UPDATED] +3 scraping endpoints
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scrape_and_populate.py    [NEW] Scraping script
â”‚   â””â”€â”€ SCRAPING_GUIDE.md         [NEW] Documentation
â””â”€â”€ requirements.txt               [UPDATED] +4 dependencies
```

## Benefits

1. **Automated Knowledge Base**: Populate from ASTU website automatically
2. **Fresh Content**: Re-scrape to keep content updated
3. **Flexible Sources**: Admin can add from any URL or manually
4. **Same Infrastructure**: Uses existing Supabase, Gemini, RAG setup
5. **Scalable**: Chunk large docs, handle many pages
6. **Production Ready**: Rate limiting, error handling, logging

Your RAG system is now fully equipped to handle both manual admin entries and automated web scraping! ğŸš€
